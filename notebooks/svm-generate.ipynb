{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lefunes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lefunes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from logger import log, debug\n",
    "from cleaner import clean_corpus_basic\n",
    "from reader import read_files\n",
    "from svm_utils import get_best_tokens_dummy, transform_sentence, get_score\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import svm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Hiperparametros -----------------\n",
    "\n",
    "train_to_test = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Leyendo archivos en panda...]\n",
      "[Archivos Leidos...]\n",
      "[Usando cleaner basico]\n",
      "[Limpiando el corpus...]\n",
      "[Usando 8 threads ...]\n",
      "[El corpus tiene 20000 rows]\n",
      "[Luego de filtrar el corpus tiene 20000 rows]\n"
     ]
    }
   ],
   "source": [
    "# Leemos los archivos\n",
    "corpus = clean_corpus_basic(read_files(\"../dataset/\", [\"negative\", \"positive\"]))\n",
    "\n",
    "# Filtramos por longitud\n",
    "debug(\"[El corpus tiene \" + str(len(corpus)) + \" rows]\")\n",
    "\n",
    "#corpus[\"length\"] = corpus[\"content\"].map(lambda x: len(x.split(\" \")))\n",
    "#corpus = corpus[(corpus[\"length\"] >= min_lenght) & (corpus[\"length\"] <= max_length)]\n",
    "\n",
    "debug(\"[Luego de filtrar el corpus tiene \" + str(len(corpus)) + \" rows]\")\n",
    "\n",
    "#Shuffleamos el corpus\n",
    "corpus = shuffle(corpus)\n",
    "corpus.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supertrain(train_set):\n",
    "    train_set_splitted = np.array_split(train_set, 5)\n",
    "    max_classifier = (None, 0)\n",
    "\n",
    "    # la idea es hacer un cross-check, con 5 arrays\n",
    "    splitted_len = len(train_set_splitted)\n",
    "    for i in range(splitted_len):\n",
    "        no_better_count = 0\n",
    "        for _ in range(20): #Hacemos 20 iteraciones\n",
    "            if(no_better_count > 5): #Despues de 3 iteraciones sin encontrar mejor, paramos\n",
    "                break\n",
    "                \n",
    "            train_subset = train_set_splitted[0:i] + train_set_splitted[(i + 1): splitted_len]\n",
    "            train_subset = [item for sublist in train_subset for item in sublist]\n",
    "            np.random.shuffle(train_subset)\n",
    "\n",
    "            np.random.shuffle(train_set_splitted[i])\n",
    "                \n",
    "            # Creamos el clasificador y lo evaluamos\n",
    "            unzipped = list(zip(*train_subset)) \n",
    "            classifier = svm.SVC(gamma='scale', probability=True)\n",
    "            classifier.fit(unzipped[0], unzipped[1])\n",
    "            acc = get_score(classifier, train_set_splitted[i])\n",
    "\n",
    "            # Vemos si encontramos un clasificador mejor\n",
    "            if(acc > max_classifier[1]):\n",
    "                log(\"[   Se encontro un clasificador mejor \" + str(acc * 100) + \"% ]\")\n",
    "                max_classifier = (classifier, acc) # Guardamos el mejor clasificador\n",
    "            no_better_count += 1 # Si no encontramos mejor, aumentamos en uno\n",
    "\n",
    "    return max_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entrenando con parametro 4 3]\n",
      "[   Se encontro un clasificador mejor 89.25% ]\n",
      "[   Se encontro un clasificador mejor 89.40625% ]\n",
      "[   Accuaracy real 88.9% ]\n",
      "[Fin del entrenamiento]\n"
     ]
    }
   ],
   "source": [
    "results_svm = []\n",
    "\n",
    "each_q = 4\n",
    "vector_size = 3\n",
    "\n",
    "log(\"[Entrenando con parametro \" + str(each_q) + \" \" + str(vector_size) + \"]\")\n",
    "best_tokens = get_best_tokens_dummy(corpus, each_q)\n",
    "best_tokens.to_pickle(\"best_tokens_svm.bin\")\n",
    "\n",
    "corpus[\"vector\"] = corpus[\"content\"].map(lambda x: transform_sentence(x, best_tokens, vector_size))\n",
    "\n",
    "# Dividimos el corpus\n",
    "\n",
    "subset = corpus[['vector','rate']]\n",
    "featuresets = [tuple(x) for x in subset.values]\n",
    "train, test = np.split(featuresets, [int(train_to_test * len(featuresets))])\n",
    "clasif = supertrain(train)\n",
    "\n",
    "real_acc = get_score(clasif[0], test)\n",
    "\n",
    "log(\"[   Accuaracy real \" + str(real_acc * 100) + \"% ]\")\n",
    "\n",
    "results_svm.append((each_q, vector_size, real_acc, clasif[0], clasif[1]))\n",
    "log(\"[Fin del entrenamiento]\")\n",
    "\n",
    "f = open(\"svm-no-restricted-4-3.bin\", \"wb+\")\n",
    "pickler = pickle.Pickler(f)\n",
    "pickler.dump(clasif[0])\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
