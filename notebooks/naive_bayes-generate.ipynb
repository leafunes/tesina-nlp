{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from logger import log, debug\n",
    "from cleaner import clean_corpus_basic\n",
    "from reader import read_files\n",
    "from nb_utils import get_best_tokens_dummy\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------- Hiperparametros -----------------\n",
    "\n",
    "min_each_q = 60\n",
    "max_each_q = 70\n",
    "interval = 1\n",
    "min_lenght, max_length = 4, 8\n",
    "train_to_test = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Leyendo archivos en panda...]\n",
      "[Archivos Leidos...]\n",
      "[Usando cleaner basico]\n",
      "[Limpiando el corpus...]\n",
      "[Usando 8 threads ...]\n",
      "[El corpus tiene 20000 rows]\n",
      "[Luego de filtrar el corpus tiene 9164 rows]\n"
     ]
    }
   ],
   "source": [
    "# Leemos los archivos\n",
    "corpus = clean_corpus_basic(read_files(\"../dataset/\", [\"negative\", \"positive\"]))\n",
    "\n",
    "# Filtramos por longitud\n",
    "debug(\"[El corpus tiene \" + str(len(corpus)) + \" rows]\")\n",
    "\n",
    "corpus[\"length\"] = corpus[\"content\"].map(lambda x: len(x.split(\" \")))\n",
    "corpus = corpus[(corpus[\"length\"] >= min_lenght) & (corpus[\"length\"] <= max_length)]\n",
    "\n",
    "debug(\"[Luego de filtrar el corpus tiene \" + str(len(corpus)) + \" rows]\")\n",
    "\n",
    "#Shuffleamos el corpus\n",
    "corpus = shuffle(corpus)\n",
    "corpus.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_nb_dictionaty(cleaned_sentence, best_tokens):\n",
    "    to_ret = {}\n",
    "    for item in best_tokens.iteritems():\n",
    "        to_ret[item[0]] = 0\n",
    "    tokens = nltk.word_tokenize(cleaned_sentence, 'spanish')\n",
    "    for token in tokens:\n",
    "        if (token in to_ret):\n",
    "            to_ret[token] += 1\n",
    "    return to_ret\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supertrain(train_set):\n",
    "    train_set_splitted = np.array_split(train_set, 5)\n",
    "    max_classifier = (None, 0)\n",
    "\n",
    "    # la idea es hacer un cross-check, con 5 arrays\n",
    "    splitted_len = len(train_set_splitted)\n",
    "    for i in range(splitted_len):\n",
    "        no_better_count = 0\n",
    "        for _ in range(100): #Hacemos 100 iteraciones\n",
    "            if(no_better_count > 10): #Despues de 10 iteraciones sin encontrar mejor, paramos\n",
    "                break\n",
    "                \n",
    "            train_subset = train_set_splitted[0:i] + train_set_splitted[(i + 1): splitted_len]\n",
    "            train_subset = [item for sublist in train_subset for item in sublist]\n",
    "            np.random.shuffle(train_subset)\n",
    "\n",
    "            np.random.shuffle(train_set_splitted[i])\n",
    "                \n",
    "            # Creamos el clasificador y lo evaluamos\n",
    "            classifier = nltk.NaiveBayesClassifier.train(train_subset)\n",
    "            acc = nltk.classify.accuracy(classifier, train_set_splitted[i])\n",
    "\n",
    "            # Vemos si encontramos un clasificador mejor\n",
    "            if(acc > max_classifier[1]):\n",
    "                log(\"[   Se encontro un clasificador mejor \" + str(acc * 100) + \"% ]\")\n",
    "                max_classifier = (classifier, acc) # Guardamos el mejor clasificador\n",
    "            no_better_count += 1 # Si no encontramos mejor, aumentamos en uno\n",
    "\n",
    "    return max_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entrenando con parametro 50]\n",
      "[   Se encontro un clasificador mejor 92.63803680981594% ]\n",
      "[   Accuaracy real 93.12602291325696% ]\n",
      "[Fin del entrenamiento]\n"
     ]
    }
   ],
   "source": [
    "results_nb = []\n",
    "p = 50\n",
    "\n",
    "log(\"[Entrenando con parametro \" + str(p) + \"]\")\n",
    "best_tokens = get_best_tokens_dummy(corpus, p)\n",
    "best_tokens.to_pickle(\"best_tokens_nb.bin\")\n",
    "corpus[\"nb_dict\"] = corpus[\"content\"].map(lambda x: to_nb_dictionaty(x, best_tokens))\n",
    "\n",
    "# Dividimos el corpus\n",
    "\n",
    "subset = corpus[['nb_dict','rate']]\n",
    "featuresets = [tuple(x) for x in subset.values]\n",
    "train, test = np.split(featuresets, [int(train_to_test * len(featuresets))])\n",
    "clasif = supertrain(train)\n",
    "\n",
    "real_acc = nltk.classify.accuracy(clasif[0], test)\n",
    "\n",
    "log(\"[   Accuaracy real \" + str(real_acc * 100) + \"% ]\")\n",
    "\n",
    "results_nb.append((p, real_acc, clasif[0], clasif[1]))\n",
    "log(\"[Fin del entrenamiento]\")\n",
    "\n",
    "f = open(\"naive-bayes-no-restricted-50.bin\", \"wb+\")\n",
    "pickler = pickle.Pickler(f)\n",
    "pickler.dump(clasif[0])\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
